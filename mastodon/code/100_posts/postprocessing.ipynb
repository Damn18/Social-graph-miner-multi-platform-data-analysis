{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b65a36",
   "metadata": {},
   "source": [
    "# Post-processing\n",
    "\n",
    "- check number of posts per month \n",
    "- check english and removal of non-english text \n",
    "- check presence of duplicates + removal \n",
    "- random sample: at least 100 post per month \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0443707",
   "metadata": {},
   "source": [
    "## 1. Number of posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/dataset/100_posts/100_posts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for fname in os.listdir(folder):\n",
    "    if fname.endswith(\".json\"):\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            print(f\"{fname}: {len(data)} elements\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dce9d9",
   "metadata": {},
   "source": [
    "## 2. Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in os.listdir(folder):\n",
    "    if not fname.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    fpath = os.path.join(folder, fname)\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    unique_posts = []\n",
    "\n",
    "    for post in data:\n",
    "        post_id = post.get(\"uri\")\n",
    "        if post_id in seen:\n",
    "            duplicates.append(post)\n",
    "        else:\n",
    "            seen.add(post_id)\n",
    "            unique_posts.append(post)\n",
    "\n",
    "    if duplicates:\n",
    "        print(f\"{fname}: trovati {len(duplicates)} duplicati\")\n",
    "    else:\n",
    "        print(f\"{fname}: nessun duplicato\")\n",
    "\n",
    "    # Uncomment to overwrite files without duplicates \n",
    "    # if len(unique_posts) < len(data):\n",
    "    #     with open(fpath, \"w\", encoding=\"utf-8\") as file:\n",
    "    #         json.dump(unique_posts, file, ensure_ascii=False, indent=2)\n",
    "    #     print(f\"{fname}: sovrascritto con {len(unique_posts)} post unici\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe92744",
   "metadata": {},
   "source": [
    "# 3. Check english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, fname)\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        posts = json.load(f)\n",
    "\n",
    "    english_posts = []\n",
    "    for post in posts:\n",
    "        lang = post.get(\"language\")\n",
    "        if lang is not None:\n",
    "            is_en = (lang == \"en\")\n",
    "        else:\n",
    "            soup = BeautifulSoup(post.get(\"content\", \"\"), \"html.parser\")\n",
    "            try:\n",
    "                is_en = detect(soup.get_text()) == \"en\"\n",
    "            except:\n",
    "                is_en = False\n",
    "\n",
    "        if is_en:\n",
    "            english_posts.append(post)\n",
    "\n",
    "    # overwrite with only English posts\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(english_posts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"{fname}: {len(english_posts)} English posts out of {len(posts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5608c52",
   "metadata": {},
   "source": [
    "# Hashtag extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f: \n",
    "        data = json.load(f)\n",
    "        for element in data:\n",
    "            if \"tags\" in element:\n",
    "                for tag in element[\"tags\"]:\n",
    "                    if \"name\" in tag:\n",
    "                        hashtags.append(tag[\"name\"])\n",
    "\n",
    "                            \n",
    "\n",
    "\n",
    "new_file = os.path.join(folder, \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/hashtag_raw.json\")\n",
    "\n",
    "with open(new_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hashtags, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d441c",
   "metadata": {},
   "source": [
    "# Counting hashtags extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/hashtag_raw.json\"\n",
    "out  = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/hashtag_norm.json\"\n",
    "\n",
    "#\n",
    "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    hashtags = json.load(f)\n",
    "\n",
    "# lowercase normalization\n",
    "hashtags_nor = [tag.lower() for tag in hashtags]\n",
    "\n",
    "# frequencies\n",
    "count_ba = Counter(hashtags)\n",
    "\n",
    "\n",
    "with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(count_ba, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total hashtags: {len(hashtags_nor)}\")\n",
    "print(f\"Unique hashtags: {len(count_ba)}\")\n",
    "print(\"Top 10:\", count_ba.most_common(20))\n",
    "# print(\"\\n\")\n",
    "# for a in count_ba.most_common(100):\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03409487",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags, freqs = zip(*count_ba.most_common(20))  \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(hashtags, freqs)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top 20 Hashtags Distribution\")\n",
    "plt.xlabel(\"Hashtags\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ad430",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ba = np.array(sorted(count_ba.values(), reverse=True))\n",
    "ranks_ba = np.arange(1, len(freq_ba) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.loglog(ranks_ba, freq_ba, linestyle=\"-\", linewidth=2, color=\"blue\")  # linea continua\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Hashtag Distribution (log-log scale)\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08918cb2",
   "metadata": {},
   "source": [
    "# 1-random day data flow download \n",
    "\n",
    "The following codes allow to: \n",
    "- create a list of the 100 most used hashtags from the 1.5 year dataset sample \n",
    "- select a random day in the time range \n",
    "- download all the posts for such a day containing at least one of the 100 hashtags \n",
    "- compute the cumulative distribution of such hashtags \n",
    "\n",
    "The idea here is to retrieve the most used hashtags in a random day (cause the z-score approach showed a zipf distribution and failed to explain data). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbcda95",
   "metadata": {},
   "source": [
    "### 1. Creation list of 100 most used hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/hashtag_norm.json\"\n",
    "out_file = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/top100_hashtags.json\"\n",
    "\n",
    "with open(in_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    hashtag_counts = json.load(f)\n",
    "\n",
    "top100 = sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "top100_hashtags = [tag for tag, _ in top100]\n",
    "\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(top100_hashtags, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/hashtag_norm.json\"\n",
    "out_file = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/top100_hashtags.json\"\n",
    "\n",
    "with open(in_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    hashtag_counts = json.load(f)\n",
    "\n",
    "top100 = sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6edef4",
   "metadata": {},
   "source": [
    "### 2. Selection of random day in time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7501e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(1)\n",
    "def random_day_between(start_str, end_str):\n",
    "\n",
    "    start_date = datetime.strptime(start_str, \"%Y-%m-%d\").date()\n",
    "    end_date   = datetime.strptime(end_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "    delta_days = (end_date - start_date).days\n",
    "\n",
    "    random_offset = random.randint(0, delta_days)\n",
    "    random_date   = start_date + timedelta(days=random_offset)\n",
    "\n",
    "    return random_date.isoformat()\n",
    "\n",
    "START_STR = \"2024-02-06\"  \n",
    "END_STR   = \"2025-07-06\" \n",
    "call = random_day_between(START_STR, END_STR)\n",
    "print(call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58752c",
   "metadata": {},
   "source": [
    "### 3. Adjustment of download code to retrieve the data flow of the random day \n",
    "\n",
    "to be run as pyhton file \n",
    "\n",
    "- adjust random day selected\n",
    "- adjust output directory and best_100_hashtag file directory \n",
    "- create a .py file \n",
    "- run it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfef9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from mastodon import Mastodon\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "### CONFIG \n",
    "OUTPUT_DIR  = ''\n",
    "load_dotenv()\n",
    "INSTANCE= 'https://mastodon.social'\n",
    "HASHTAG = 'climatechange'\n",
    "START= datetime(2024, 12, 8, tzinfo=timezone.utc)\n",
    "END= datetime(2025, 12, 8, tzinfo=timezone.utc)\n",
    "ACCESS_TOKEN = os.getenv('MASTODON_TOKEN')\n",
    "\n",
    "### DIRECTORY \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "mastodon = Mastodon(access_token=ACCESS_TOKEN, api_base_url=INSTANCE)\n",
    "seen_ids = set()\n",
    "\n",
    "### EXTRACTION  \n",
    "current_day = START\n",
    "while current_day < END:\n",
    "    posts_saved = 0\n",
    "    month_file_path = os.path.join(OUTPUT_DIR, f\"{current_day.year}-{current_day.month:02d}.json\")\n",
    "\n",
    "    with open(month_file_path, 'a', encoding='utf-8') as fout:\n",
    "        while posts_saved < 5:\n",
    "            rand_hour = random.randint(0, 23)\n",
    "            rand_minute = random.randint(0, 59)\n",
    "            rand_second = random.randint(0, 59)\n",
    "            random_dt = current_day.replace(hour=rand_hour, minute=rand_minute, second=rand_second)\n",
    "            \n",
    "            posts = mastodon.timeline_hashtag(\n",
    "                hashtag=HASHTAG,\n",
    "                max_id=random_dt,\n",
    "                limit=1\n",
    "            )\n",
    "            time.sleep(2)  \n",
    "            if posts:\n",
    "                s = posts[0]\n",
    "                if current_day <= s.created_at < (current_day + timedelta(days=1)) and s.id not in seen_ids:\n",
    "                    \n",
    "                    fout.write(json.dumps(posts, ensure_ascii=False, default=str) + \"\\n\")\n",
    "                    seen_ids.add(s.id)\n",
    "                    posts_saved += 1\n",
    "    current_day += timedelta(days=1)\n",
    "\n",
    "print(f\"Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb1f60",
   "metadata": {},
   "source": [
    "### 4. Hashtag retrival + plot/statistics\n",
    "\n",
    "Adjust in/out file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "in_file  = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/dataset/100_posts/1_day/2024-12-08.jsonl\"  \n",
    "out_file = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/1_day/hashtags.json\"\n",
    "\n",
    "hashtags = []\n",
    "\n",
    "with open(in_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)  \n",
    "        if \"tags\" in data:\n",
    "            for tag in data[\"tags\"]:\n",
    "                if \"name\" in tag:\n",
    "                    hashtags.append(tag[\"name\"])\n",
    "\n",
    "counts = Counter(hashtags)\n",
    "\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dict(counts), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(counts)} unique hashtags, {sum(counts.values())} total hashtags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_file = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/1_day/hashtags.json\"\n",
    "top100_file = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/top100_hashtags.json\"\n",
    "out_plot    = \"/home/damn/Documents/PROJECTS/THESIS/Social-graph-miner-multi-platform-data-analysis/mastodon/code/hashtag/100_posts/top100_cumulata.png\"\n",
    "\n",
    "with open(counts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_counts = json.load(f)\n",
    "\n",
    "with open(top100_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    top100 = json.load(f)\n",
    "\n",
    "# build dataframe preserving the order of the list\n",
    "data = [(tag, all_counts.get(tag, 0)) for tag in top100]\n",
    "df = pd.DataFrame(data, columns=[\"Hashtag\", \"Count\"])\n",
    "\n",
    "# compute cumulative %\n",
    "df[\"Cumulata %\"] = df[\"Count\"].cumsum() / df[\"Count\"].sum() * 100\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df[\"Hashtag\"], df[\"Cumulata %\"], marker=\"o\", color=\"blue\", linewidth=2)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Hashtag\")\n",
    "plt.ylabel(\"Cumulata %\")\n",
    "plt.title(\"Cumulative distribution Mastodon\")\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "\n",
    "# save\n",
    "plt.savefig(out_plot)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
